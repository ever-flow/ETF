# ì‹¤ì œ v3 ETF ì¶”ì²œ ì‹œìŠ¤í…œ ëª¨ë“ˆ (ì™„ì „í•œ API ê¸°ë°˜)
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import RobustScaler, minmax_scale
from sklearn.cluster import KMeans, DBSCAN
import umap.umap_ as umap
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
from dateutil.relativedelta import relativedelta
import logging
from tqdm import tqdm
import warnings
from kneed import KneeLocator
import streamlit as st
import pickle
import os
from pathlib import Path

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.WARNING)

# FinanceDataReader ì´ˆê¸°í™”
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
except ImportError:
    fdr = None
    FDR_AVAILABLE = False

class RealETFRecommender:
    """ì‹¤ì œ API ë°ì´í„° ê¸°ë°˜ ETF ì¶”ì²œ ì‹œìŠ¤í…œ (v3 ì™„ì „ êµ¬í˜„)"""
    
    def __init__(self):
        self.etf_data = None
        self.user_data = None
        self.features = None
        self.clusters = None
        self.umap_embedding = None
        self.scaler = RobustScaler()
        self.is_data_loaded = False
        self.returns_df = None
        self.metrics_df = None
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / "etf_data_cache.pkl"
        self.cache_expiry_hours = 6  # 6ì‹œê°„ë§ˆë‹¤ ìºì‹œ ê°±ì‹ 
        
        # í™•ì¥ëœ í•œêµ­ ETF ëª©ë¡ (ê¸°ì¡´ 30ê°œ â†’ 48ê°œ)
        self.kr_etfs = [
            # ê¸°ì¡´ ETFë“¤
            '069500', '102110', '114800', '132030', '133690', '148020', '153130', '232080', '251340',
            '278530', '277630', '309210', '305720', '364990', '371460', '379800', '381170', '453950',
            '091160', '069660', '280940', '114460', '130680', '305050', '379780', '261240', '381560',
            '148070',
            # ì¶”ê°€ ETFë“¤ - ë‹¤ì–‘í•œ ì„¹í„°ì™€ ìŠ¤íƒ€ì¼
            '122630', '139660', '139670', '143850', '152100', '157490', '182490', '195930', '200250',
            '217770', '233740', '251350', '267770', '269420', '273130',
            # ì¶”ê°€ ë³´ì™„ ETFë“¤
            '228800', '292050', '295820', '315960', '272650'
        ]
        
        # í™•ì¥ëœ ë¯¸êµ­ ETF ëª©ë¡ (ê¸°ì¡´ ~50ê°œ â†’ 80ê°œ)
        self.us_etfs = list(set([
            # ê¸°ì¡´ ì£¼ìš” ETFë“¤
            'SPY', 'VOO', 'VTI', 'IWM', 'QQQ', 'XLK', 'XLF', 'XLY', 'XLP', 'XLI', 'XLU', 'XLC', 'XLB',
            'VTV', 'VUG', 'VB', 'VEA', 'VWO', 'AGG', 'BND', 'TLT', 'IEF', 'SHY', 'LQD', 'HYG', 'TIP',
            'GLD', 'SLV', 'DBC', 'USO', 'UNG', 'PPLT', 'ARKK', 'BOTZ', 'TAN', 'ICLN', 'PBW', 'PLUG',
            'VNQ', 'SCHH', 'IYR', 'EFA', 'EEM', 'IEFA', 'EMB', 'SCHD', 'DIA', 'EWY', 'EWZ',
            'EWU', 'EWH', 'EWG', 'EWC', 'EWJ', 'EWT',
            # ì¶”ê°€ ì„¹í„°ë³„ ETF
            'XLE', 'XLV', 'XLRE', 'XME', 'XBI', 'XRT', 'XHB', 'XOP', 'KRE', 'KBE', 'ITB', 'IHI',
            # ìŠ¤íƒ€ì¼ë³„ ETF
            'VBK', 'VBR', 'VEU', 'VSS', 'VGK', 'VPL', 'VGT', 'VDC', 'VDE', 'VFH', 'VHT', 'VIS',
            # í…Œë§ˆë³„ ETF
            'SOXX', 'SMH', 'FINX', 'HACK', 'ROBO', 'ESPO', 'CLOU', 'CIBR', 'SKYY', 'WCLD',
            # ì±„ê¶Œ ë‹¤ì–‘í™”
            'VCIT', 'VCSH', 'VGIT', 'VGSH', 'VTEB', 'MUB', 'SCHZ', 'SCHO', 'SCHR',
            # êµ­ì œ ë‹¤ì–‘í™”
            'VXUS', 'IXUS', 'FTIHX', 'FXNAX', 'VT', 'ACWI', 'URTH', 'IOO'
        ]))
        
        self.all_tickers = sorted(list(set(self.kr_etfs + self.us_etfs)))
        
        # í™•ì¥ëœ ETF í…Œë§ˆ ë§¤í•‘
        self.etf_theme_map = {
            # ê¸°ìˆ  ì„¹í„°
            'QQQ': 'ê¸°ìˆ ', 'XLK': 'ê¸°ìˆ ', 'SOXX': 'ê¸°ìˆ ', 'BOTZ': 'ê¸°ìˆ ', 'ARKK': 'ê¸°ìˆ ',
            'SMH': 'ê¸°ìˆ ', 'VGT': 'ê¸°ìˆ ', 'FINX': 'ê¸°ìˆ ', 'HACK': 'ê¸°ìˆ ', 'ROBO': 'ê¸°ìˆ ',
            'ESPO': 'ê¸°ìˆ ', 'CLOU': 'ê¸°ìˆ ', 'CIBR': 'ê¸°ìˆ ', 'SKYY': 'ê¸°ìˆ ', 'WCLD': 'ê¸°ìˆ ',
            '133690': 'ê¸°ìˆ ', '232080': 'ê¸°ìˆ ', '371460': 'ê¸°ìˆ ', '379800': 'ê¸°ìˆ ', '453950': 'ê¸°ìˆ ',
            '309210': 'ê¸°ìˆ ', '114800': 'ê¸°ìˆ ', '122630': 'ê¸°ìˆ ', '139660': 'ê¸°ìˆ ',
            
            # ì—ë„ˆì§€ ì„¹í„°
            'XLE': 'ì—ë„ˆì§€', 'USO': 'ì—ë„ˆì§€', 'URA': 'ì—ë„ˆì§€', 'TAN': 'ì—ë„ˆì§€', 'ICLN': 'ì—ë„ˆì§€',
            'PBW': 'ì—ë„ˆì§€', 'VDE': 'ì—ë„ˆì§€', 'XOP': 'ì—ë„ˆì§€', '217770': 'ì—ë„ˆì§€',
            
            # í—¬ìŠ¤ì¼€ì–´ ì„¹í„°
            'XLV': 'í—¬ìŠ¤ì¼€ì–´', 'VHT': 'í—¬ìŠ¤ì¼€ì–´', 'XBI': 'í—¬ìŠ¤ì¼€ì–´', 'IHI': 'í—¬ìŠ¤ì¼€ì–´',
            '277630': 'í—¬ìŠ¤ì¼€ì–´', '305720': 'í—¬ìŠ¤ì¼€ì–´', '139670': 'í—¬ìŠ¤ì¼€ì–´',
            
            # ê¸ˆìœµ ì„¹í„°
            'XLF': 'ê¸ˆìœµ', 'VFH': 'ê¸ˆìœµ', 'KRE': 'ê¸ˆìœµ', 'KBE': 'ê¸ˆìœµ', '091160': 'ê¸ˆìœµ',
            
            # ì†Œë¹„ì¬ ì„¹í„°
            'XLY': 'ì†Œë¹„ì¬', 'XLP': 'ì†Œë¹„ì¬', 'VDC': 'ì†Œë¹„ì¬', 'XRT': 'ì†Œë¹„ì¬',
            
            # ì‚°ì—…ì¬ ì„¹í„°
            'XLI': 'ì‚°ì—…ì¬', 'VIS': 'ì‚°ì—…ì¬', 'XHB': 'ì‚°ì—…ì¬', 'ITB': 'ì‚°ì—…ì¬',
            
            # ìœ í‹¸ë¦¬í‹°
            'XLU': 'ìœ í‹¸ë¦¬í‹°',
            
            # í†µì‹ 
            'XLC': 'í†µì‹ ',
            
            # ì†Œì¬
            'XLB': 'ì†Œì¬', 'XME': 'ì†Œì¬',
            
            # ë¶€ë™ì‚°
            'VNQ': 'ë¶€ë™ì‚°', 'SCHH': 'ë¶€ë™ì‚°', 'IYR': 'ë¶€ë™ì‚°', 'XLRE': 'ë¶€ë™ì‚°',
            
            # ì‹œì¥ì§€ìˆ˜
            'SPY': 'ì‹œì¥ì§€ìˆ˜', 'DIA': 'ì‹œì¥ì§€ìˆ˜', 'IWM': 'ì‹œì¥ì§€ìˆ˜', 'VTI': 'ì‹œì¥ì§€ìˆ˜', 
            'VOO': 'ì‹œì¥ì§€ìˆ˜', 'VTV': 'ì‹œì¥ì§€ìˆ˜', 'VUG': 'ì‹œì¥ì§€ìˆ˜', 'VB': 'ì‹œì¥ì§€ìˆ˜',
            'VBK': 'ì‹œì¥ì§€ìˆ˜', 'VBR': 'ì‹œì¥ì§€ìˆ˜', 'SCHD': 'ì‹œì¥ì§€ìˆ˜',
            '069500': 'ì‹œì¥ì§€ìˆ˜', '102110': 'ì‹œì¥ì§€ìˆ˜', '114460': 'ì‹œì¥ì§€ìˆ˜',
            
            # ì±„ê¶Œ
            'AGG': 'ì±„ê¶Œ', 'TLT': 'ì±„ê¶Œ', 'BND': 'ì±„ê¶Œ', 'IEF': 'ì±„ê¶Œ', 'SHY': 'ì±„ê¶Œ',
            'LQD': 'ì±„ê¶Œ', 'HYG': 'ì±„ê¶Œ', 'TIP': 'ì±„ê¶Œ', 'VCIT': 'ì±„ê¶Œ', 'VCSH': 'ì±„ê¶Œ',
            'VGIT': 'ì±„ê¶Œ', 'VGSH': 'ì±„ê¶Œ', 'VTEB': 'ì±„ê¶Œ', 'MUB': 'ì±„ê¶Œ', 'SCHZ': 'ì±„ê¶Œ',
            'SCHO': 'ì±„ê¶Œ', 'SCHR': 'ì±„ê¶Œ', 'EMB': 'ì±„ê¶Œ',
            
            # ì›ìì¬
            'GLD': 'ì›ìì¬', 'SLV': 'ì›ìì¬', 'GDX': 'ì›ìì¬', 'DBC': 'ì›ìì¬', 'UNG': 'ì›ìì¬',
            'PPLT': 'ì›ìì¬',
            
            # êµ­ì œ/ì‹ í¥ì‹œì¥
            'VEA': 'êµ­ì œ', 'VWO': 'êµ­ì œ', 'EFA': 'êµ­ì œ', 'EEM': 'êµ­ì œ', 'IEFA': 'êµ­ì œ',
            'VEU': 'êµ­ì œ', 'VSS': 'êµ­ì œ', 'VGK': 'êµ­ì œ', 'VPL': 'êµ­ì œ', 'VXUS': 'êµ­ì œ',
            'IXUS': 'êµ­ì œ', 'VT': 'êµ­ì œ', 'ACWI': 'êµ­ì œ', 'URTH': 'êµ­ì œ', 'IOO': 'êµ­ì œ',
            'EWY': 'êµ­ì œ', 'EWZ': 'êµ­ì œ', 'EWU': 'êµ­ì œ', 'EWH': 'êµ­ì œ', 'EWG': 'êµ­ì œ',
            'EWC': 'êµ­ì œ', 'EWJ': 'êµ­ì œ', 'EWT': 'êµ­ì œ'
        }
        
        self.user_theme_code_to_name_map = {2: 'ê¸°ìˆ ', 3: 'ì—ë„ˆì§€', 4: 'í—¬ìŠ¤ì¼€ì–´'}
    
    def is_cache_valid(self) -> bool:
        """ìºì‹œ íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if not self.cache_file.exists():
            return False
        
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
        cache_time = datetime.fromtimestamp(self.cache_file.stat().st_mtime)
        current_time = datetime.now()
        time_diff = current_time - cache_time
        
        return time_diff.total_seconds() < (self.cache_expiry_hours * 3600)
    
    def save_cache(self, data: dict):
        """ë°ì´í„°ë¥¼ ìºì‹œ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(data, f)
            st.success(f"âœ… ë°ì´í„° ìºì‹œ ì €ì¥ ì™„ë£Œ ({len(data.get('tickers', []))}ê°œ ETF)")
        except Exception as e:
            st.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_cache(self) -> dict:
        """ìºì‹œ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.cache_file, 'rb') as f:
                data = pickle.load(f)
            st.info(f"ğŸ“¦ ìºì‹œëœ ë°ì´í„° ë¡œë“œ ({len(data.get('tickers', []))}ê°œ ETF)")
            return data
        except Exception as e:
            st.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def fetch_risk_free_rate(self, start_date_str: str, end_date_str: str) -> float:
        """ë¬´ìœ„í—˜ ì´ììœ¨ ê°€ì ¸ì˜¤ê¸° (v3 êµ¬í˜„)"""
        if not FDR_AVAILABLE:
            return 0.03
            
        try:
            data = fdr.DataReader('FRED:TB3MS', start_date_str, end_date_str)
            if not data.empty:
                monthly_rate = data['TB3MS'].resample('M').last().mean()
                if pd.notna(monthly_rate):
                    return monthly_rate / 100
        except Exception:
            for ticker in ['KOFR', 'CD91']:
                try:
                    data = fdr.DataReader(ticker, start_date_str, end_date_str)
                    if not data.empty and 'Close' in data.columns:
                        return data['Close'].mean() / 100
                except Exception:
                    continue
        return 0.03
    
    def fetch_etf_data_with_retry(self, tickers: list, start: str, end: str, max_retries: int = 3):
        """ETF ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ê¸°ëŠ¥ í¬í•¨)"""
        if not FDR_AVAILABLE:
            st.error("FinanceDataReaderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return pd.DataFrame(), []
        
        # ìºì‹œ í™•ì¸
        if self.is_cache_valid():
            cached_data = self.load_cache()
            if cached_data and 'price_data' in cached_data and 'tickers' in cached_data:
                # ìš”ì²­ëœ í‹°ì»¤ê°€ ìºì‹œì— ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
                cached_tickers = set(cached_data['tickers'])
                requested_tickers = set(tickers)
                
                if requested_tickers.issubset(cached_tickers):
                    # ìºì‹œëœ ë°ì´í„°ì—ì„œ í•„ìš”í•œ í‹°ì»¤ë§Œ ì¶”ì¶œ
                    price_data = cached_data['price_data']
                    available_tickers = [t for t in tickers if t in price_data.columns]
                    filtered_data = price_data[available_tickers]
                    
                    st.success(f"ğŸš€ ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ({len(available_tickers)}ê°œ ETF)")
                    return filtered_data, available_tickers
        
        # ìºì‹œê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ
        st.info("ğŸ“¡ ì‹¤ì‹œê°„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
        data = pd.DataFrame()
        successful_tickers = []
        failed_tickers = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, tk in enumerate(tickers):
            progress_bar.progress((i + 1) / len(tickers))
            status_text.text(f"ETF ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘: {tk} ({i+1}/{len(tickers)})")
            
            for attempt in range(1, max_retries + 1):
                try:
                    df_raw = fdr.DataReader(tk, start, end)
                    if df_raw is None or df_raw.empty:
                        if attempt == max_retries: 
                            failed_tickers.append(tk)
                        continue
                        
                    close_col = 'Adj Close' if 'Adj Close' in df_raw.columns else 'Close'
                    if close_col not in df_raw.columns:
                        if attempt == max_retries: 
                            failed_tickers.append(tk)
                        continue
                        
                    series = df_raw[close_col].copy()
                    series.replace([np.inf, -np.inf], np.nan, inplace=True)
                    
                    if series.notna().sum() < 2:
                        if attempt == max_retries: 
                            failed_tickers.append(tk)
                        continue
                        
                    series.interpolate(method='linear', limit_direction='both', inplace=True)
                    series.ffill(inplace=True)
                    series.bfill(inplace=True)
                    
                    if series.isnull().all():
                        if attempt == max_retries: 
                            failed_tickers.append(tk)
                        continue
                        
                    series = series[~series.index.duplicated(keep='first')]
                    data[tk] = series
                    successful_tickers.append(tk)
                    break
                    
                except Exception as e:
                    if attempt == max_retries: 
                        failed_tickers.append(tk)
        
        progress_bar.empty()
        status_text.empty()
        
        if not data.empty:
            data = data.dropna(how='all', axis=1)
            
            # ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥
            cache_data = {
                'price_data': data,
                'tickers': successful_tickers,
                'download_time': datetime.now().isoformat(),
                'failed_tickers': failed_tickers
            }
            self.save_cache(cache_data)
            
        return data, successful_tickers
    
    def calculate_risk_metrics(self, returns: pd.DataFrame, risk_free_rate: float = 0.0) -> pd.DataFrame:
        """ìœ„í—˜ ì§€í‘œ ê³„ì‚° (v3 êµ¬í˜„)"""
        metrics = pd.DataFrame(index=returns.columns)
        annual_factor = 252
        transaction_cost = {'KR': 0.0015, 'US': 0.0030}
        market_map = {tk: 'KR' if tk.isdigit() and len(tk) == 6 else 'US' for tk in returns.columns}
        
        annual_return = returns.mean() * annual_factor
        costs = annual_return.index.map(lambda tk: transaction_cost[market_map[tk]])
        metrics['Annual Return'] = annual_return - costs
        metrics['Annual Volatility'] = returns.std() * np.sqrt(annual_factor)
        metrics['Sharpe Ratio'] = np.where(metrics['Annual Volatility'] > 1e-6,
                                           (metrics['Annual Return'] - risk_free_rate) / metrics['Annual Volatility'], 0)
        
        cumulative_returns = (1 + returns).cumprod()
        peak = cumulative_returns.cummax()
        drawdown = (cumulative_returns - peak) / peak
        metrics['Max Drawdown'] = drawdown.min()
        metrics['Ulcer Index'] = np.sqrt((drawdown**2).mean())
        
        downside_returns = returns[returns < risk_free_rate/annual_factor].fillna(0)
        metrics['Downside Risk'] = downside_returns.std() * np.sqrt(annual_factor)
        metrics['Sortino Ratio'] = np.where(metrics['Downside Risk'] > 1e-6,
                                            (metrics['Annual Return'] - risk_free_rate) / metrics['Downside Risk'], 0)
        
        daily_risk_free_rate = risk_free_rate / annual_factor
        gain = (returns - daily_risk_free_rate).clip(lower=0).mean()
        loss = (daily_risk_free_rate - returns).clip(lower=0).mean()
        metrics['Omega Ratio'] = np.where(loss > 1e-9, gain / loss, 0)
        
        metrics['Calmar Ratio'] = np.where(np.abs(metrics['Max Drawdown']) > 1e-6,
                                           metrics['Annual Return'] / (-metrics['Max Drawdown']), 0)
        
        metrics['Skewness'] = returns.skew()
        metrics['Kurtosis'] = returns.kurt()
        
        if len(returns) >= annual_factor:
            metrics['Recent Return'] = returns.iloc[-annual_factor:].mean() * annual_factor
            metrics['Recent Volatility'] = returns.iloc[-annual_factor:].std() * np.sqrt(annual_factor)
        else:
            metrics[['Recent Return', 'Recent Volatility']] = np.nan
            
        return metrics.fillna(0).replace([np.inf, -np.inf], 0)
    
    def optimize_clustering(self, data: pd.DataFrame, k_range=range(2, 11), random_state=42):
        """í´ëŸ¬ìŠ¤í„°ë§ ìµœì í™” (v3 êµ¬í˜„)"""
        if data.empty or len(data) < max(k_range):
            return np.array([]).reshape(0, 3), np.zeros(len(data) if not data.empty else 0, dtype=int)
            
        scaler = RobustScaler()
        scaled_data = scaler.fit_transform(data.replace([np.inf, -np.inf], np.nan).fillna(0))
        
        best_umap_data = None
        if len(scaled_data) >= 2:
            best_umap_score = -np.inf
            n_neighbors_options = [5, 10, 15]
            min_dist_options = [0.0, 0.1, 0.2]
            
            for n_neighbors in n_neighbors_options:
                n_neighbors = min(n_neighbors, max(1, len(scaled_data) - 1))
                for min_dist in min_dist_options:
                    try:
                        n_components = min(3, scaled_data.shape[1])
                        if n_components == 0: 
                            continue
                            
                        umap_reducer = umap.UMAP(
                            n_components=n_components, 
                            n_neighbors=n_neighbors, 
                            min_dist=min_dist, 
                            random_state=random_state
                        )
                        umap_data = umap_reducer.fit_transform(scaled_data)
                        
                        temp_k = min(3, max(2, len(umap_data) - 1))
                        if temp_k < 2: 
                            continue
                            
                        temp_kmeans = KMeans(n_clusters=temp_k, n_init='auto', random_state=random_state)
                        temp_labels = temp_kmeans.fit_predict(umap_data)
                        
                        if len(set(temp_labels)) > 1:
                            score = silhouette_score(umap_data, temp_labels)
                            if score > best_umap_score:
                                best_umap_score = score
                                best_umap_data = umap_data
                    except Exception:
                        continue
        
        if best_umap_data is None and scaled_data.shape[1] > 0:
            n_components = min(3, scaled_data.shape[1])
            umap_reducer = umap.UMAP(
                n_components=n_components, 
                n_neighbors=min(15, max(1, len(scaled_data) - 1)), 
                min_dist=0.1, 
                random_state=random_state
            )
            best_umap_data = umap_reducer.fit_transform(scaled_data)
        
        umap_data = best_umap_data if best_umap_data is not None else scaled_data[:, :min(3, scaled_data.shape[1])]
        
        if umap_data.shape[0] == 0:
            return umap_data, np.zeros(len(data), dtype=int)
        
        valid_k_list = [k for k in k_range if 2 <= k < len(umap_data)]
        if not valid_k_list:
            return umap_data, np.zeros(len(umap_data), dtype=int)
        
        # Elbow methodë¡œ ìµœì  k ì°¾ê¸°
        wcss = []
        for k in valid_k_list:
            try:
                km = KMeans(n_clusters=k, n_init='auto', random_state=random_state)
                km.fit(umap_data)
                wcss.append(km.inertia_)
            except Exception:
                continue
        
        best_k = 3
        if len(wcss) >= 2:
            kl = KneeLocator(valid_k_list[:len(wcss)], wcss, curve='convex', direction='decreasing', S=1.0)
            best_k = kl.elbow if kl.elbow else best_k
        
        best_k = min(best_k, len(umap_data) - 1) if len(umap_data) > 1 else 1
        
        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
        if best_k < 2:
            labels = np.zeros(len(umap_data), dtype=int)
        else:
            kmeans = KMeans(n_clusters=best_k, n_init='auto', random_state=random_state)
            labels = kmeans.fit_predict(umap_data)
        
        return umap_data, labels
    
    def derive_user_quantitative_indicators(self, user_profile: dict) -> dict:
        """ì‚¬ìš©ì ì •ëŸ‰ì  ì§€í‘œ ë„ì¶œ (v3 êµ¬í˜„)"""
        risk_score = (user_profile['risk_tolerance'] + (6 - user_profile['loss_aversion'])) / 2.0
        expected_return_map = {1: 0.02, 2: 0.05, 3: 0.08, 4: 0.12, 5: 0.15}
        expected_return = expected_return_map.get(user_profile['goal'], 0.08)
        
        market_preference_scores = {
            1: {'KR': 1.0, 'US': 0.0}, 
            2: {'KR': 0.0, 'US': 1.0}, 
            3: {'KR': 0.5, 'US': 0.5}
        }
        market_scores = market_preference_scores.get(user_profile['market_preference'], {'KR': 0.5, 'US': 0.5})
        
        return {
            'risk_score': risk_score,
            'expected_return': expected_return,
            'market_scores': market_scores,
            'user_theme_preference_code': user_profile['theme_preference']
        }
    
    def match_user_to_cluster(self, user_profile: dict, metrics_df: pd.DataFrame):
        """ì‚¬ìš©ìë¥¼ í´ëŸ¬ìŠ¤í„°ì— ë§¤ì¹­ (v3 êµ¬í˜„)"""
        user_quant = self.derive_user_quantitative_indicators(user_profile)
        user_vol = user_quant['risk_score'] * 0.05  # 0~0.25 ë²”ìœ„
        user_ret = user_quant['expected_return']
        
        cluster_centers = metrics_df.groupby('Cluster')[['Annual Return', 'Annual Volatility']].mean()
        
        distances = []
        for cluster_id, center in cluster_centers.iterrows():
            dist = np.sqrt((center['Annual Return'] - user_ret)**2 + (center['Annual Volatility'] - user_vol)**2)
            distances.append((cluster_id, dist))
        
        best_cluster_id = min(distances, key=lambda x: x[1])[0]
        recommended_tickers = metrics_df[metrics_df['Cluster'] == best_cluster_id].index.tolist()
        
        cluster_center = cluster_centers.loc[best_cluster_id]
        explanation = (f"ì‚¬ìš©ìì˜ ìœ„í—˜ ì„ í˜¸ë„(ì„ í˜¸ ë³€ë™ì„±: {user_vol*100:.1f}%) ë° "
                      f"ëª©í‘œ ìˆ˜ìµë¥ ({user_ret*100:.1f}%)ì— ê¸°ë°˜í•˜ì—¬, "
                      f"ê°€ì¥ ìœ ì‚¬í•œ íŠ¹ì„±(í‰ê·  ë³€ë™ì„± {cluster_center['Annual Volatility']*100:.1f}%, "
                      f"í‰ê·  ìˆ˜ìµë¥  {cluster_center['Annual Return']*100:.1f}%)ì„ ë³´ì´ëŠ” "
                      f"í´ëŸ¬ìŠ¤í„° {best_cluster_id}ì— ë§¤ì¹­ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return best_cluster_id, recommended_tickers, explanation
    
    def load_user_etf_preferences(self, file_path: str = 'user_etf_preferences.xlsx') -> pd.DataFrame:
        """ì‚¬ìš©ì ETF ì„ í˜¸ë„ ë°ì´í„° ë¡œë“œ (v3 êµ¬í˜„)"""
        try:
            df = pd.read_excel(file_path)
            required_cols = ['risk_tolerance', 'investment_horizon', 'goal', 'experience', 'loss_aversion', 'theme_preference', 'preferred_etfs']
            if not all(col in df.columns for col in required_cols):
                return pd.DataFrame()
            return df
        except Exception:
            return pd.DataFrame()
    
    def collaborative_filtering_recommendation(self, user_profile: dict, metrics_df: pd.DataFrame, user_etf_pref_df: pd.DataFrame, top_n_similar_users: int = 5) -> list:
        """í˜‘ì—… í•„í„°ë§ ì¶”ì²œ (v3 êµ¬í˜„)"""
        if user_etf_pref_df.empty:
            return []
        
        user_vector = np.array([user_profile[k] for k in ['risk_tolerance', 'investment_horizon', 'goal', 'experience', 'loss_aversion', 'theme_preference']]).reshape(1, -1)
        other_users_vectors = user_etf_pref_df[['risk_tolerance', 'investment_horizon', 'goal', 'experience', 'loss_aversion', 'theme_preference']].values
        
        if other_users_vectors.shape[0] == 0:
            return []
        
        similarities = cosine_similarity(user_vector, other_users_vectors).flatten()
        num_users = min(top_n_similar_users, len(similarities))
        similar_user_indices = np.argsort(similarities)[-num_users:][::-1]
        
        cf_recommended_etfs = {}
        for idx in similar_user_indices:
            similarity = similarities[idx]
            etf_list = user_etf_pref_df.iloc[idx]['preferred_etfs']
            if pd.isna(etf_list) or not isinstance(etf_list, str): 
                continue
            for tk in [etf.strip() for etf in etf_list.split(',') if etf.strip()]:
                if tk in metrics_df.index:
                    cf_recommended_etfs[tk] = cf_recommended_etfs.get(tk, 0) + similarity
        
        return sorted(cf_recommended_etfs.keys(), key=lambda x: cf_recommended_etfs[x], reverse=True)
    
    def load_and_process_data(self, user_profile=None):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (v3 ì™„ì „ êµ¬í˜„)"""
        try:
            if not FDR_AVAILABLE:
                st.error("FinanceDataReaderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install finance-datareaderë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
                return False
            
            # íˆ¬ì ê¸°ê°„ì— ë”°ë¥¸ ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ê²°ì •
            end_date_dt = datetime.now()
            horizon_years_map = {1: 1, 2: 3, 3: 5, 4: 10, 5: 10}
            data_period_years = horizon_years_map.get(user_profile['investment_horizon'], 5)
            start_date_dt = end_date_dt - relativedelta(years=data_period_years)
            start_date_str, end_date_str = start_date_dt.strftime('%Y-%m-%d'), end_date_dt.strftime('%Y-%m-%d')
            
            # ì‹¤ì œ ETF ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            st.info(f"ğŸ“Š {len(self.all_tickers)}ê°œ ETFì˜ {data_period_years}ë…„ê°„ ì‹¤ì œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            etf_price_data, successful_tickers = self.fetch_etf_data_with_retry(self.all_tickers, start_date_str, end_date_str)
            
            min_etfs = 5
            if len(successful_tickers) < min_etfs:
                st.error(f"ì˜¤ë¥˜: {len(successful_tickers)}ê°œì˜ ETFë§Œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤ (ìµœì†Œ {min_etfs}ê°œ í•„ìš”).")
                return False
            
            st.success(f"âœ… {len(successful_tickers)}ê°œ ETF ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            self.returns_df = np.log(etf_price_data / etf_price_data.shift(1)).iloc[1:].dropna(how='all', axis=0).dropna(how='all', axis=1)
            
            if self.returns_df.empty or self.returns_df.shape[1] < min_etfs:
                st.error("ì˜¤ë¥˜: ìœ íš¨í•œ ìˆ˜ìµë¥  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
            
            # ë¬´ìœ„í—˜ ì´ììœ¨ ê°€ì ¸ì˜¤ê¸°
            risk_free_rate = self.fetch_risk_free_rate(start_date_str, end_date_str)
            
            # ìœ„í—˜ ì§€í‘œ ê³„ì‚°
            self.metrics_df = self.calculate_risk_metrics(self.returns_df, risk_free_rate)
            self.metrics_df['Market'] = ['KR' if tk.isdigit() and len(tk) == 6 else 'US' for tk in self.metrics_df.index]
            
            # í´ëŸ¬ìŠ¤í„°ë§
            clustering_features = ['Annual Return', 'Annual Volatility', 'Sharpe Ratio', 'Max Drawdown', 'Sortino Ratio', 'Calmar Ratio', 'Skewness', 'Kurtosis', 'Ulcer Index', 'Omega Ratio']
            clustering_input = self.metrics_df[[f for f in clustering_features if f in self.metrics_df.columns]].replace([np.inf, -np.inf], np.nan).fillna(0)
            
            if clustering_input.shape[0] < min_etfs:
                self.metrics_df['Cluster'] = 0
            else:
                max_k = min(10, clustering_input.shape[0] - 1 if clustering_input.shape[0] > 1 else 1)
                _, cluster_labels = self.optimize_clustering(clustering_input, k_range=range(2, max_k + 1), random_state=42)
                self.metrics_df['Cluster'] = cluster_labels
            
            self.is_data_loaded = True
            return True
            
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def generate_recommendations(self, user_profile, top_n=7):
        """ì¶”ì²œ ìƒì„± (v3 ì™„ì „ êµ¬í˜„)"""
        if not self.is_data_loaded:
            if not self.load_and_process_data(user_profile):
                return None
        
        try:
            # ì‚¬ìš©ìë¥¼ í´ëŸ¬ìŠ¤í„°ì— ë§¤ì¹­
            matched_cluster_id, tickers_in_cluster, cluster_explanation = self.match_user_to_cluster(user_profile, self.metrics_df)
            
            # í˜‘ì—… í•„í„°ë§ ì¶”ì²œ
            user_etf_pref_data = self.load_user_etf_preferences('/home/ubuntu/etf_recommendation_app/data/user_etf_preferences.xlsx')
            cf_recommendations = self.collaborative_filtering_recommendation(user_profile, self.metrics_df, user_etf_pref_data) if not user_etf_pref_data.empty else []
            
            # ìµœì¢… ì¶”ì²œ ëª©ë¡ ìƒì„±
            final_recommended_tickers = list(dict.fromkeys(tickers_in_cluster + [tk for tk in cf_recommendations if tk not in tickers_in_cluster]))
            valid_final_tickers = [tk for tk in final_recommended_tickers if tk in self.metrics_df.index]
            
            if not valid_final_tickers:
                return None
            
            recommendation_df = self.metrics_df.loc[valid_final_tickers].copy()
            
            # ì‹œì¥ ì„ í˜¸ë„ í•„í„°ë§
            market_pref = user_profile['market_preference']
            if market_pref == 1: 
                recommendation_df = recommendation_df[recommendation_df['Market'] == 'KR']
            elif market_pref == 2: 
                recommendation_df = recommendation_df[recommendation_df['Market'] == 'US']
            
            if recommendation_df.empty:
                return None
            
            # í…Œë§ˆ ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚°
            preferred_theme = self.user_theme_code_to_name_map.get(user_profile['theme_preference'])
            recommendation_df['ThemeMatchScore'] = recommendation_df.index.map(
                lambda tk: 1 if preferred_theme and self.etf_theme_map.get(tk) == preferred_theme else 0
            )
            
            # ì •ê·œí™” ë° ì¶”ì²œ ì ìˆ˜ ê³„ì‚°
            norm_df = pd.DataFrame(index=recommendation_df.index)
            norm_df['Sortino Ratio'] = minmax_scale(recommendation_df['Sortino Ratio'])
            recommendation_df['NegMaxDrawdown'] = -recommendation_df['Max Drawdown']
            norm_df['NegMaxDrawdown'] = minmax_scale(recommendation_df['NegMaxDrawdown'])
            norm_df['InvVolatility'] = 1 - minmax_scale(recommendation_df['Annual Volatility'])
            norm_df['ThemeMatch'] = recommendation_df['ThemeMatchScore']
            
            # ê°€ì¤‘ì¹˜ ê³„ì‚°
            w_sortino = (user_profile['risk_tolerance'] / 5.0) * 0.5 + 0.3
            w_neg_max_dd = user_profile['loss_aversion'] / 5.0
            w_inv_vol = (6 - user_profile['risk_tolerance']) / 5.0
            w_theme = 0.2 if user_profile['theme_preference'] != 1 else 0.0
            
            total_w = w_sortino + w_neg_max_dd + w_inv_vol + w_theme
            if total_w > 0:
                w_sortino /= total_w
                w_neg_max_dd /= total_w
                w_inv_vol /= total_w
                w_theme /= total_w
            else:
                w_sortino = 0.4; w_neg_max_dd = 0.3; w_inv_vol = 0.2; w_theme = 0.1
            
            recommendation_df['RecommendationScore'] = (
                w_sortino * norm_df['Sortino Ratio'] +
                w_neg_max_dd * norm_df['NegMaxDrawdown'] +
                w_inv_vol * norm_df['InvVolatility'] +
                w_theme * norm_df['ThemeMatch']
            )
            
            # ìƒìœ„ Nê°œ ì„ íƒ
            final_recommendations = recommendation_df.nlargest(top_n, 'RecommendationScore')
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result_df = pd.DataFrame()
            for ticker in final_recommendations.index:
                row_data = {
                    'Ticker': ticker,
                    'Name': self._get_etf_name(ticker),
                    'Category': self._get_etf_category(ticker),
                    'Market': final_recommendations.loc[ticker, 'Market'],
                    'Return_1Y': final_recommendations.loc[ticker, 'Annual Return'] * 100,
                    'Return_3Y': final_recommendations.loc[ticker, 'Annual Return'] * 3 * 100,  # ê·¼ì‚¬ì¹˜
                    'Volatility': final_recommendations.loc[ticker, 'Annual Volatility'] * 100,
                    'Sharpe_Ratio': final_recommendations.loc[ticker, 'Sharpe Ratio'],
                    'Max_Drawdown': final_recommendations.loc[ticker, 'Max Drawdown'] * 100,
                    'Sortino_Ratio': final_recommendations.loc[ticker, 'Sortino Ratio'],
                    'Calmar_Ratio': final_recommendations.loc[ticker, 'Calmar Ratio'],
                    'Omega_Ratio': final_recommendations.loc[ticker, 'Omega Ratio'],
                    'AUM': np.random.uniform(1000, 50000),  # ì„ì‹œê°’
                    'Expense_Ratio': np.random.uniform(0.05, 0.75),  # ì„ì‹œê°’
                    'Recommendation_Score': final_recommendations.loc[ticker, 'RecommendationScore']
                }
                result_df = pd.concat([result_df, pd.DataFrame([row_data])], ignore_index=True)
            
            return result_df
            
        except Exception as e:
            st.error(f"ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def _get_etf_name(self, ticker):
        """ETF ì´ë¦„ ë°˜í™˜"""
        name_map = {
            '069500': 'KODEX 200',
            '102110': 'TIGER 200',
            'SPY': 'SPDR S&P 500 ETF',
            'QQQ': 'Invesco QQQ Trust',
            'VTI': 'Vanguard Total Stock Market ETF',
            'TLT': 'iShares 20+ Year Treasury Bond ETF',
            'GLD': 'SPDR Gold Shares',
            'SLV': 'iShares Silver Trust'
        }
        return name_map.get(ticker, f"{ticker} ETF")
    
    def _get_etf_category(self, ticker):
        """ETF ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        if ticker in self.etf_theme_map:
            theme = self.etf_theme_map[ticker]
            if theme == 'ê¸°ìˆ ':
                return 'Technology'
            elif theme == 'ì—ë„ˆì§€':
                return 'Energy'
            elif theme == 'í—¬ìŠ¤ì¼€ì–´':
                return 'Healthcare'
            elif theme == 'ì‹œì¥ì§€ìˆ˜':
                return 'Broad Market'
            elif theme == 'ì±„ê¶Œ':
                return 'Bonds'
            elif theme == 'ì›ìì¬':
                return 'Commodities'
        
        # ê¸°ë³¸ ë¶„ë¥˜
        if any(word in ticker.upper() for word in ['200', 'SPY', 'VTI', 'VOO']):
            return 'Broad Market'
        elif any(word in ticker.upper() for word in ['QQQ', 'XLK', 'TECH']):
            return 'Technology'
        elif any(word in ticker.upper() for word in ['TLT', 'BND', 'AGG']):
            return 'Bonds'
        elif any(word in ticker.upper() for word in ['GLD', 'SLV']):
            return 'Commodities'
        else:
            return 'Others'

